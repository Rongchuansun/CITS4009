df_cluster <- df[groups == i, cols_to_print]
if (nrow(df_cluster) > max_rows) {
print(df_cluster[1:max_rows, ])
print(paste("... and", nrow(df_cluster) - max_rows, "more rows"))
} else {
print(df_cluster)
}
}
}
groups <- cutree(pfit, k=7)
cols_to_print <- c('category', 'subscribers', 'video_views_for_the_last_30_days')
print_clusters(youtube2, groups, cols_to_print, max_rows = 10)
fviz_cluster(list(data = scaled_df, cluster = groups),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
kmClustering.ch <- kmeansruns(scaled_df, krange=1:10, criterion="ch")
kmClustering.asw <- kmeansruns(scaled_df, krange=1:10, criterion="asw")
kmCritframe <- data.frame(k=1:10, ch=kmClustering.ch$crit,
asw=kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x=k, y=ch)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(kmCritframe, aes(x=k, y=asw)) +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="ASW") + theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)
kmClusters1 <- kmeans(scaled_df, 2, nstart=100, iter.max=100)
kmClusters2 <- kmeans(scaled_df, 9, nstart=100, iter.max=100)
# Create a data frame to store the cluster assignments and original data
clustered_data <- data.frame(Cluster = kmClusters2$cluster, scaled_df)
# Create an empty list to store the data frames for each cluster
cluster_data_list <- list()
# Loop through each cluster and extract 10 rows of data for each
for (cluster_num in 1:9) {
cluster_data <- clustered_data[clustered_data$Cluster == cluster_num, ]
cluster_data_list[[cluster_num]] <- head(cluster_data, 10)
}
# Print the first 10 rows of each cluster
for (cluster_num in 1:9) {
cat(paste("Cluster ", cluster_num, ":\n"))
print(cluster_data_list[[cluster_num]])
}
fviz_cluster(kmClusters1, data = scaled_df,
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
fviz_cluster(kmClusters2, data = scaled_df,
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(ggplot2)
library(lubridate)
library(ROCR)
library(knitr)
library(rpart)
library(rpart.plot)
library(pander)
library(ROCit)
library(grid)
library(gridExtra)
library(grDevices)
library(fpc)
library(ggpubr)
library(factoextra)
library(readxl)
youtube <- read.csv('Global YouTube Statistics.csv')
summary(youtube$video_views_for_the_last_30_days)     #there are 56 NAs
youtube <-filter(youtube, !is.na(video_views_for_the_last_30_days))   #excluding NAs
# replace NA with "other" for category and channel_type columns
youtube$category[is.na(youtube$category)] <- "Other"
youtube$channel_type[is.na(youtube$channel_type)] <- "Other"
# replace nan with other" category and channel_type columns
youtube$category[youtube$category == "nan"] <- "Other"
youtube$channel_type[youtube$channel_type == "nan"] <- "Other"
# calculate the median of subscribers_for_last_30_days
median_value <- median(youtube$subscribers_for_last_30_days, na.rm = TRUE)
# replace NaN with the median
youtube$subscribers_for_last_30_days[youtube$subscribers_for_last_30_days == "NaN"] <- median_value
replaceTopBottomOutliers <- function(data, column, lower_percentile, upper_percentile) {
# Calculate the quantiles for the specified column
thresholds <- quantile(data[[column]], probs = c(lower_percentile, upper_percentile), na.rm = TRUE)
# Replace values in the specified column that are above the upper threshold
data[[column]][data[[column]] > thresholds[2]] <- median(data[[column]], na.rm = TRUE)
# Replace values in the specified column that are below the lower threshold
data[[column]][data[[column]] < thresholds[1]] <- median(data[[column]], na.rm = TRUE)
cat("Min after ETL:", min(data[[column]], na.rm = TRUE), "\n")
cat("Max after ETL:", max(data[[column]], na.rm = TRUE), "\n")
return(data)
}
# Example usage:
youtube <- replaceTopBottomOutliers(youtube, "highest_monthly_earnings", 0.25, 0.8)
youtube <- replaceTopBottomOutliers(youtube, "lowest_monthly_earnings", 0.2, 0.8)
youtube <- replaceTopBottomOutliers(youtube, "highest_yearly_earnings", 0.25, 0.8)
youtube <- replaceTopBottomOutliers(youtube, "lowest_yearly_earnings", 0.2, 0.75)
youtube <- replaceTopBottomOutliers(youtube, "uploads", 0.1, 1)
youtube <- replaceTopBottomOutliers(youtube, "subscribers", 0, 1)
youtube <- replaceTopBottomOutliers(youtube, "video.views", 0.04, 1)
youtube <- replaceTopBottomOutliers(youtube, "subscribers_for_last_30_days", 0.2, 0.9)
youtube <- replaceTopBottomOutliers(youtube, "created_date", 0.1, 0.9)
youtube$vv <- ifelse(youtube$video_views_for_the_last_30_days > 80000000, 1, 0)   #1 for hige video views, 2 for low video views
table(youtube$vv)
hist(youtube$video_views_for_the_last_30_days, breaks = 200,
xlab = 'video views for the last 30 days', main = 'Histogram of video views for the last 30 days')
youtube$created <- as.POSIXct("2023-10-20")
year(youtube$created) <- as.integer(youtube$created_year)
youtube$created_month <- as.factor(youtube$created_month)
youtube$created_month <- factor(youtube$created_month, levels = c('Jan', 'Feb', 'Mar',
'Apr', 'May', 'Jun',
'Jul', 'Aug', 'Sep',
'Oct', 'Nov', 'Dec'),
labels = c('01', '02', '03', '04', '05', '06',
'07', '08', '09', '10', '11', '12'))
month(youtube$created) <- as.integer(youtube$created_month)
mday(youtube$created) <- as.integer(youtube$created_date)       #combine year, month and day
youtube$created_time <- as.POSIXct("2023-10-20") - youtube$created      #calculate total opening days
youtube$created_time <- as.numeric(youtube$created_time)
youtube <- rename(youtube, subscribers_30 = 'subscribers_for_last_30_days',
edu = 'Gross.tertiary.education.enrollment....')      #rename column names
Vars <- c('subscribers', 'video.views', 'uploads', 'video_views_rank',
'country_rank', 'channel_type_rank','lowest_monthly_earnings',
'highest_monthly_earnings', 'lowest_yearly_earnings',
'highest_yearly_earnings', 'subscribers_30', 'edu', 'Population',
'Unemployment.rate', 'created_time')
correlation <- cor(youtube$video_views_for_the_last_30_days, youtube[, Vars], use = 'pairwise.complete.obs')
correlation[, order(correlation, decreasing = T)]
candidate.columns <- c('subscribers', 'video.views', 'category', 'uploads',
'channel_type', 'lowest_monthly_earnings',
'highest_monthly_earnings', 'lowest_yearly_earnings',
'highest_yearly_earnings', 'subscribers_30', 'created_time')
predict1.columns <- c('subscribers', 'uploads', 'channel_type', 'created_time')
predict2.columns <- c('lowest_yearly_earnings', 'video.views', 'subscribers_30', 'category')
outcome <- 'vv'
youtube <- youtube[, c(outcome, candidate.columns)]
catVars <- candidate.columns[sapply(youtube[,candidate.columns],class) %in% c('factor','character')]
numericVars <- candidate.columns[sapply(youtube[,candidate.columns],class) %in% c('numeric','integer')]
set.seed(9924)
youtube$rgroup <- runif(dim(youtube)[1])
TrainAll <- subset(youtube, rgroup<=0.9)
Test <- subset(youtube, rgroup>0.9)     #split datasets into a training set and a test set
useForCal <- rbinom(n=dim(TrainAll)[1], size=1, prob=0.1)>0
Cal <- subset(TrainAll, useForCal)
Train <- subset(TrainAll, !useForCal)     #split TrainAll into a training set and a calibration set
pos <- '1'
mkPredC <- function(outCol, varCol, appCol) {
pPos <- sum(outCol == pos) / length(outCol)
naTab <- table(as.factor(outCol[is.na(varCol)]))
pPosWna <- (naTab/sum(naTab))[pos]
vTab <- table(as.factor(outCol), varCol)
pPosWv <- (vTab[pos, ] + 1.0e-3*pPos) / (colSums(vTab) + 1.0e-3)
pred <- pPosWv[appCol]
pred[is.na(appCol)] <- pPosWna
pred[is.na(pred)] <- pPos
pred
}             #function to repeat model building
for(v in catVars) {
pi <- paste('pred', v, sep='')
Train[,pi] <- mkPredC(Train[,outcome], Train[,v], Train[,v])
Cal[,pi] <- mkPredC(Train[,outcome], Train[,v], Cal[,v])
Test[,pi] <- mkPredC(Train[,outcome], Train[,v], Test[,v])
}             #predict for all categorical variables
calcAUC <- function(predcol,outcol) {
perf <- performance(prediction(predcol,outcol==pos),'auc')
as.numeric(perf@y.values)
}
for(v in catVars) {
pi <- paste('pred', v, sep='')
aucTrain <- calcAUC(Train[,pi], Train[,outcome])
if (aucTrain >= 0.5) {
aucCal <- calcAUC(Cal[,pi], Cal[,outcome])
print(sprintf(
"%s: trainAUC: %4.3f; calibrationAUC: %4.3f",
pi, aucTrain, aucCal))
}
}
mkPredN <- function(outCol, varCol, appCol) {
cuts <- unique(
quantile(varCol, probs=seq(0, 1, 0.1), na.rm=T))
varC <- cut(varCol,cuts)
appC <- cut(appCol,cuts)
mkPredC(outCol,varC,appC)
}
for(v in numericVars) {
pi <- paste('pred', v, sep='')
Train[,pi] <- mkPredN(Train[,outcome], Train[,v], Train[,v])
Cal[,pi] <- mkPredN(Train[,outcome], Train[,v], Cal[,v])
Test[,pi] <- mkPredN(Train[,outcome], Train[,v], Test[,v])
aucTrain <- calcAUC(Train[,pi], Train[,outcome])
if(aucTrain >= 0.5) {
aucCal <- calcAUC(Cal[,pi], Cal[,outcome])
print(sprintf(
"%s: trainAUC: %4.3f; calibrationAUC: %4.3f",
pi, aucTrain, aucCal))
}
}
all.vars <- c(catVars, numericVars)
models.auc <- data.frame(model.type = 'univariate',
model.name = all.vars,
train.auc = sapply(all.vars, function(v){pi <- paste('pred',v,sep='');
calcAUC(Train[,pi], Train[,outcome])}),
cal.auc = sapply(all.vars, function(v){pi <- paste('pred',v,sep='');
calcAUC(Cal[,pi],Cal[,outcome])}))
kable(models.auc[order(-models.auc$cal.auc), ])
logLikelihood <- function(ytrue, ypred, epsilon=1e-6) {
sum(ifelse(ytrue==pos, log(ypred+epsilon), log(1-ypred-epsilon)), na.rm=T)
}
logNull <- logLikelihood(
Cal[,outcome], sum(Cal[,outcome]==pos)/nrow(Cal)
)
cat(logNull)
selCatVars <- c()
minDrop <- 1
for (v in catVars) {
pi <- paste('pred', v, sep='')
devDrop <- 2*(logLikelihood(Cal[,outcome], Cal[,pi]) - logNull)
if (devDrop >= minDrop) {
print(sprintf("%s, deviance reduction: %g", pi, devDrop))
selCatVars <- c(selCatVars, pi)
}
}
selNumVars <- c()
minDrop <- 1
for (v in numericVars) {
pi <- paste('pred', v, sep='')
devDrop <- 2*(logLikelihood(Cal[,outcome], Cal[,pi]) - logNull)
if (devDrop >= minDrop) {
print(sprintf("%s, deviance reduction: %g", pi, devDrop))
selNumVars <- c(selNumVars, pi)
}
}
logLikelihood <- function(ytrue, ypred, epsilon=1e-6) {
sum(ifelse(ytrue, log(ypred+epsilon), log(1-ypred+epsilon)), na.rm=T)
}
performanceMeasures <- function(ytrue, ypred, model.name = "model", threshold=0.5) {
dev.norm <- -2 * logLikelihood(ytrue, ypred)/length(ypred)
cmat <- table(actual = ytrue, predicted = ypred >= threshold)
accuracy <- sum(diag(cmat)) / sum(cmat)
precision <- cmat[2, 2] / sum(cmat[, 2])
recall <- cmat[2, 2] / sum(cmat[2, ])
f1 <- 2 * precision * recall / (precision + recall)
data.frame(model = model.name, precision = precision,
recall = recall, f1 = f1, dev.norm = dev.norm)
}
panderOpt <- function(){
panderOptions("plain.ascii", TRUE)
panderOptions("keep.trailing.zeros", TRUE)
panderOptions("table.style", "simple")
}
pretty_perf_table <- function(model, xtrain, ytrain,
xtest, ytest, threshold=0.5) {
panderOpt()
perf_justify <- "lrrrr"
pred_train <- predict(model, newdata=xtrain)
pred_test <- predict(model, newdata=xtest)
trainperf_df <- performanceMeasures(
ytrain, pred_train, model.name="training", threshold=threshold)
testperf_df <- performanceMeasures(
ytest, pred_test, model.name="test", threshold=threshold)
perftable <- rbind(trainperf_df, testperf_df)
pandoc.table(perftable, justify = perf_justify)
}
plot_roc <- function(predcol1, outcol1, predcol2, outcol2){
roc_1 <- rocit(score=predcol1, class=outcol1==pos)
roc_2 <- rocit(score=predcol2, class=outcol2==pos)
plot(roc_1, col = c("blue","green"), lwd = 3,
legend = FALSE,YIndex = FALSE, values = TRUE, asp=1)
lines(roc_2$TPR ~ roc_2$FPR, lwd = 3,
col = c("red","green"), asp=1)
legend("bottomright", col = c("blue","red", "green"),
c("Test Data", "Training Data", "Null Model"), lwd = 2)
}
fV1 <- paste(outcome,'>0 ~ ', paste(predict1.columns, collapse=' + '), sep='')
tmodel_play <- rpart(fV1, data=Train)
rpart.plot(tmodel_play)
print(calcAUC(predict(tmodel_play, newdata=Train), Train[,outcome]))
print(calcAUC(predict(tmodel_play, newdata=Test), Test[,outcome]))
print(calcAUC(predict(tmodel_play, newdata=Cal), Cal[,outcome]))
pretty_perf_table(tmodel_play, Train[predict1.columns], Train[,outcome]==pos,
Test[predict1.columns], Test[,outcome]==pos)
plot_roc(predict(tmodel_play, newdata=Test), Test[[outcome]],
predict(tmodel_play, newdata=Train), Train[[outcome]])
fV2 <- paste(outcome,'>0 ~ ', paste(predict2.columns, collapse=' + '), sep='')
tmodel_earning <- rpart(fV2, data=Train)
rpart.plot(tmodel_earning)
print(calcAUC(predict(tmodel_earning, newdata=Train), Train[,outcome]))
print(calcAUC(predict(tmodel_earning, newdata=Test), Test[,outcome]))
print(calcAUC(predict(tmodel_earning, newdata=Cal), Cal[,outcome]))
pretty_perf_table(tmodel_earning, Train[predict2.columns], Train[,outcome]==pos,
Test[predict2.columns], Test[,outcome]==pos)
plot_roc(predict(tmodel_earning, newdata=Test), Test[[outcome]],
predict(tmodel_earning, newdata=Train), Train[[outcome]])
predict1.columns_log <- c('subscribers', 'video.views', 'uploads', 'created_time')
fL1 <- paste(outcome, paste(predict1.columns_log, collapse=" + "), sep=" ~ ")
gmodel_play <- glm(fL1, data=Train, family=binomial(link="logit"))
print(calcAUC(predict(gmodel_play, newdata=Train), Train[,outcome]))
print(calcAUC(predict(gmodel_play, newdata=Test), Test[,outcome]))
print(calcAUC(predict(gmodel_play, newdata=Cal), Cal[,outcome]))
Train$gpred <- predict(gmodel_play, newdata=Train, type="response")
Test$gpred <- predict(gmodel_play, newdata=Test, type="response")
ggplot(Train, aes(x=gpred, color=as.factor(vv), linetype=as.factor(vv))) +
geom_density(size=1.5) +
theme(text=element_text(size=20))
perf <- prediction(Train$gpred, Train$vv)
precObj <- performance(perf, measure="prec")
recObj <- performance(perf, measure="rec")
thresh <- (precObj@x.values)[[1]] # threshold
precision <- (precObj@y.values)[[1]] # precision
recall <- (recObj@y.values)[[1]] # recall
ROCdf <- data.frame(threshold=thresh, precision=precision, recall=recall)
# Null probability
pnull <- mean(as.numeric(Train$vv))
p1 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim=c(0,5) ) + labs(y="Enrichment rate")
p2 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=recall)) +
coord_cartesian(xlim = c(0,0.05))
grid.arrange(p1, p2, nrow = 2)
plot_roc(predict(gmodel_play, newdata=Test), Test[[outcome]],
predict(gmodel_play, newdata=Train), Train[[outcome]])
fL2 <- paste(outcome, paste(predict2.columns, collapse=" + "), sep=" ~ ")
gmodel_earning <- glm(fL2, data=Train, family=binomial(link="logit"))
print(calcAUC(predict(gmodel_earning, newdata=Train), Train[,outcome]))
print(calcAUC(predict(gmodel_earning, newdata=Test), Test[,outcome]))
print(calcAUC(predict(gmodel_earning, newdata=Cal), Cal[,outcome]))
Train$gpred <- predict(gmodel_earning, newdata=Train, type="response")
Test$gpred <- predict(gmodel_earning, newdata=Test, type="response")
ggplot(Train, aes(x=gpred, color=as.factor(vv), linetype=as.factor(vv))) +
geom_density(size=1.5) +
theme(text=element_text(size=20))
perf <- prediction(Train$gpred, Train$vv)
precObj <- performance(perf, measure="prec")
recObj <- performance(perf, measure="rec")
thresh <- (precObj@x.values)[[1]] # threshold
precision <- (precObj@y.values)[[1]] # precision
recall <- (recObj@y.values)[[1]] # recall
ROCdf <- data.frame(threshold=thresh, precision=precision, recall=recall)
# Null probability
pnull <- mean(as.numeric(Train$vv))
p1 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=precision/pnull)) +
coord_cartesian(xlim = c(0,0.05), ylim=c(0,5) ) + labs(y="Enrichment rate")
p2 <- ggplot(ROCdf, aes(x=threshold)) + geom_line(aes(y=recall)) +
coord_cartesian(xlim = c(0,0.05))
grid.arrange(p1, p2, nrow = 2)
plot_roc(predict(gmodel_earning, newdata=Test), Test[[outcome]],
predict(gmodel_earning, newdata=Train), Train[[outcome]])
pred_tplay <- prediction(predict(tmodel_play, newdata=Test), Test[[outcome]])
pred_gplay <- prediction(predict(gmodel_play, newdata=Test), Test[[outcome]])
pred_tearn <- prediction(predict(tmodel_earning, newdata=Test), Test[[outcome]])
pred_pearn <- prediction(predict(gmodel_earning, newdata=Test), Test[[outcome]])
preds <- list(pred_tplay, pred_gplay, pred_tearn, pred_pearn)
model.names <- c('decision tree_channel metrics', 'logistic regression_channel metrics',
'decision tree_earning estimates', 'logistic regression_video earning')
roc.df <- data.frame()
for(i in 1:length(model.names)){
pred <- preds[[i]]
model.name <- model.names[i]
perf <- performance( pred, "tpr", "fpr" )
roc.df <- rbind(roc.df, data.frame(
'fpr'=unlist(perf@x.values),
'tpr'=unlist(perf@y.values),
'threshold'=unlist(perf@alpha.values),
'model'=model.name))
}
ggplot(roc.df, aes(x=fpr, y=tpr, group=model, color=model))+
geom_line(size = 1) +
geom_abline(intercept = 0, slope = 1, size = 1,
color = 'green', linetype='dashed') +
theme_bw()
youtube2 <- read.csv('Global YouTube Statistics.csv')
youtube2 <- youtube2[, c('rank', 'category', 'subscribers', 'video.views',
'video_views_for_the_last_30_days',
'lowest_monthly_earnings', 'highest_monthly_earnings')]
youtube2 <-filter(youtube2, !is.na(video_views_for_the_last_30_days))   #excluding NA
# replace NA with "other" for category and channel_type columns
youtube2$category[is.na(youtube2$category)] <- "Other"
# replace nan with other" category and channel_type columns
youtube2$category[youtube2$category == "nan"] <- "Other"
youtube2 <- replaceTopBottomOutliers(youtube2, "highest_monthly_earnings", 0.02, 1)
youtube2 <- replaceTopBottomOutliers(youtube2, "lowest_monthly_earnings", 0.15, 1)
youtube2 <- replaceTopBottomOutliers(youtube2, "subscribers", 0, 1)
youtube2 <- replaceTopBottomOutliers(youtube2, "video.views", 0.02, 1)
youtube2 <- replaceTopBottomOutliers(youtube2, "video_views_for_the_last_30_days", 0.05, 1)
vars.to.use <- colnames(youtube2)[3:7]
scaled_df <- scale(youtube2[,vars.to.use])
d <- dist(scaled_df, method="euclidean")
pfit <- hclust(d, method="ward.D2")   # perform hierarchical clustering
plot(pfit, labels=youtube2$Youtuber, main="Cluster Dendrogram for Youtube data",
cex = 0.3, hang = -1)
# Function to return the squared Euclidean distance of two given points x and y
sqr_euDist <- function(x, y) {
sum((x - y)^2)
}
wss <- function(clustermat) {
c0 <- colMeans(clustermat)
sum(apply( clustermat, 1, FUN=function(row) {sqr_euDist(row, c0)} ))
}
# Function to calculate the total WSS.
wss_total <- function(scaled_df, labels) {
wss.sum <- 0
k <- length(unique(labels))
for (i in 1:k)
wss.sum <- wss.sum + wss(subset(scaled_df, labels == i))
wss.sum
}
# Function to calculate total sum of squared (TSS) distance
tss <- function(scaled_df) {
wss(scaled_df)
}
# Function to return the CH indices
CH_index <- function(scaled_df, kmax, method="kmeans") {
if (!(method %in% c("kmeans", "hclust")))
stop("method must be one of c('kmeans', 'hclust')")
npts <- nrow(scaled_df)
wss.value <- numeric(kmax)
wss.value[1] <- wss(scaled_df)
if (method == "kmeans") {
# kmeans
for (k in 2:kmax) {
clustering <- kmeans(scaled_df, k, nstart=10, iter.max=100)
wss.value[k] <- clustering$tot.withinss
}
} else {
# hclust
d <- dist(scaled_df, method="euclidean")
pfit <- hclust(d, method="ward.D2")
for (k in 2:kmax) {
labels <- cutree(pfit, k=k)
wss.value[k] <- wss_total(scaled_df, labels)
}
}
bss.value <- tss(scaled_df) - wss.value # this is a vector
B <- bss.value / (0:(kmax-1)) # also a vector
W <- wss.value / (npts - 1:kmax) # also a vector
data.frame(k = 1:kmax, CH_index = B/W, WSS = wss.value)
}
# calculate the CH criterion
crit.df <- CH_index(scaled_df, 10, method="hclust")
fig1 <- ggplot(crit.df, aes(x=k, y=CH_index)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(crit.df, aes(x=k, y=WSS), color="blue") +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)
print_clusters <- function(df, groups, cols_to_print, max_rows = 10) {
Ngroups <- max(groups)
for (i in 1:Ngroups) {
print(paste("cluster", i))
df_cluster <- df[groups == i, cols_to_print]
if (nrow(df_cluster) > max_rows) {
print(df_cluster[1:max_rows, ])
print(paste("... and", nrow(df_cluster) - max_rows, "more rows"))
} else {
print(df_cluster)
}
}
}
groups <- cutree(pfit, k=7)
cols_to_print <- c('category', 'subscribers', 'video_views_for_the_last_30_days')
print_clusters(youtube2, groups, cols_to_print, max_rows = 10)
fviz_cluster(list(data = scaled_df, cluster = groups),
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
kmClustering.ch <- kmeansruns(scaled_df, krange=1:10, criterion="ch")
kmClustering.asw <- kmeansruns(scaled_df, krange=1:10, criterion="asw")
kmCritframe <- data.frame(k=1:10, ch=kmClustering.ch$crit,
asw=kmClustering.asw$crit)
fig1 <- ggplot(kmCritframe, aes(x=k, y=ch)) +
geom_point() + geom_line(colour="red") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="CH index") + theme(text=element_text(size=20))
fig2 <- ggplot(kmCritframe, aes(x=k, y=asw)) +
geom_point() + geom_line(colour="blue") +
scale_x_continuous(breaks=1:10, labels=1:10) +
labs(y="ASW") + theme(text=element_text(size=20))
grid.arrange(fig1, fig2, nrow=1)
kmClusters1 <- kmeans(scaled_df, 2, nstart=100, iter.max=100)
kmClusters2 <- kmeans(scaled_df, 9, nstart=100, iter.max=100)
# Create a data frame to store the cluster assignments and original data
clustered_data <- data.frame(Cluster = kmClusters2$cluster, scaled_df)
# Create an empty list to store the data frames for each cluster
cluster_data_list <- list()
# Loop through each cluster and extract 10 rows of data for each
for (cluster_num in 1:9) {
cluster_data <- clustered_data[clustered_data$Cluster == cluster_num, ]
cluster_data_list[[cluster_num]] <- head(cluster_data, 10)
}
# Print the first 10 rows of each cluster
for (cluster_num in 1:9) {
cat(paste("Cluster ", cluster_num, ":\n"))
print(cluster_data_list[[cluster_num]])
}
fviz_cluster(kmClusters1, data = scaled_df,
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
fviz_cluster(kmClusters2, data = scaled_df,
geom = "point",
ellipse.type = "convex",
ggtheme = theme_bw()
)
